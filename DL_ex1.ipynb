{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inbalh1/ML/blob/main/DL_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zqy_C_lHYIBg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c8de892-1b69-4cec-99c8-1e1cfe124079"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "import pickle\n",
        "import os\n",
        "from google.colab import files\n",
        "import itertools\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "TRAIN_SIZE = 5000\n",
        "TEST_SIZE = 1000\n",
        "DEFAULT_NUM_OF_EPOCHS = 100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ONu4U2nVb64U"
      },
      "source": [
        "# Question 1: setup and baseline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZPm31L6RcAr0"
      },
      "source": [
        "### setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "RgoYenCEb32_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbacbb0d-cb37-4a0a-b1e7-d02799cd95a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:05<00:00, 33706005.77it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting data/cifar-10-python.tar.gz to data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True, # set to true for first run - TODO: find elegant solution\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,# set to true for first run - TODO: find elegant solution\n",
        "    transform=ToTensor(),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TLbnSbmxcCqe"
      },
      "source": [
        "### Baseline - svm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "aL5WczYWTEgy"
      },
      "outputs": [],
      "source": [
        "# HW part 1 - run svm as baseline\n",
        "\n",
        "def svm_by_type(kernel , X , y , X_test , y_test):\n",
        "  clf_SVC = SVC(kernel=kernel , decision_function_shape='ovr')\n",
        "  clf_SVC.fit(X.reshape([TRAIN_SIZE, 3 * 32 * 32]), y)\n",
        "  # TODO: should report accuracy (in the doc)\n",
        "  print (\"kernel type - \" + kernel)\n",
        "  print(\"train acc: \", clf_SVC.score(X.reshape([TRAIN_SIZE, 3 * 32 * 32]), y))\n",
        "  print(\"test acc: \", clf_SVC.score(X_test.reshape([TEST_SIZE, 3 * 32 * 32]), y_test))\n",
        " \n",
        "# This is the main function for this part \n",
        "def svm(train_sampler, test_sampler):\n",
        "  # We load everything to 1 batch\n",
        "  train_dataloader = DataLoader(training_data, batch_size=TRAIN_SIZE , sampler = train_sampler , num_workers=2)\n",
        "  test_dataloader = DataLoader(test_data, batch_size=TEST_SIZE, sampler = test_sampler , num_workers=2)\n",
        "\n",
        "  X , y = next(iter(train_dataloader))\n",
        "  X_test , y_test = next(iter(test_dataloader))\n",
        "\n",
        "  # linear SVM:\n",
        "  svm_by_type('linear' , X , y , X_test , y_test)\n",
        "\n",
        "  # rbf kernel SVM\n",
        "  svm_by_type('rbf' , X , y , X_test , y_test)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9h-xM6oJcFmR"
      },
      "source": [
        "# Question 2 - Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "WUYwte2KUemZ"
      },
      "outputs": [],
      "source": [
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(3*32*32, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def init_weights_wrapper(deviation):\n",
        "  def init_weights(m):\n",
        "    if (isinstance(m, nn.Linear)) or (isinstance(m, nn.Conv2d)):\n",
        "      torch.nn.init.normal_(m.weight , std=deviation)\n",
        "      #torch.nn.init.normal_(m.bias , std=deviation)\n",
        "      m.bias.data.fill_(0)\n",
        "  return init_weights\n",
        "\n",
        "class GridSearch():\n",
        "  def __init__(self, file_path, use_history=True):\n",
        "    self.best_params = {}\n",
        "    self.best_acc = 0\n",
        "    self.file_path = file_path\n",
        "    self.use_history = use_history\n",
        "\n",
        "  def update_params(self, step_size, momentum, deviation):\n",
        "    self.best_params['step_size'] = step_size\n",
        "    self.best_params['momentum'] = momentum\n",
        "    self.best_params['deviation'] = deviation\n",
        "\n",
        "  def write_to_file(self, accuracy, loss, time, *args):\n",
        "    obj = {args: (accuracy, loss, time)}\n",
        "    with open(self.file_path, \"ab\") as f:\n",
        "      pickle.dump(obj, f)\n",
        "\n",
        "  def get_history(self):\n",
        "    history_res = {}\n",
        "    if self.use_history and (os.path.exists(self.file_path)):\n",
        "      with open(self.file_path, \"rb\") as f:\n",
        "        objs = []\n",
        "        while 1:\n",
        "            try:\n",
        "                history_res.update(pickle.load(f))\n",
        "            except EOFError:\n",
        "                break\n",
        "    return history_res\n",
        "\n",
        "  # Config is a dict of lists that tells the values of the parameters  to run\n",
        "  def run(self, train_dataloader, test_dataloader, config, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False):\n",
        "    import time\n",
        "    history_res = self.get_history()\n",
        "    options = itertools.product(config[\"step_size\"], config[\"momentum\"], config[\"deviation\"])\n",
        "    for step_size, momentum, deviation in options:\n",
        "      # Don't run if we already know the result\n",
        "      if self.use_history and ((step_size, momentum, deviation) in history_res):\n",
        "        accuracy = history_res[(step_size, momentum, deviation)][0]\n",
        "        if accuracy > self.best_acc:\n",
        "          self.update_params(step_size, momentum, deviation)\n",
        "          self.best_acc = accuracy\n",
        "        continue\n",
        "      print(\"Starting gs for ({},{},{})\".format(step_size, momentum, deviation))\n",
        "      start = time.time()\n",
        "      model = NeuralNetwork().to(device)\n",
        "      model.apply(init_weights_wrapper(deviation))\n",
        "\n",
        "      loss_fn = nn.CrossEntropyLoss()\n",
        "      optimizer = torch.optim.SGD(model.parameters(), lr=step_size , momentum=momentum)\n",
        "      # TODO: should we check train acc or test acc (now we're checking the train)!!!!\n",
        "      # TODO2 : if we ignore the test(like now) - better not to run it\n",
        "      train_accuracy, train_loss , __, ___ = train_and_test(train_dataloader, test_dataloader, model,\n",
        "                                                    loss_fn, optimizer,\n",
        "                                                    num_of_test_batches=0, num_of_epochs=num_of_epochs, verbose=verbose)\n",
        "      tot_time = time.time() - start\n",
        "      accuracy = train_accuracy[-1] # Take last accuracy\n",
        "      loss = train_loss[-1]\n",
        "      self.write_to_file(accuracy, loss, tot_time, step_size, momentum, deviation)\n",
        "      \n",
        "      if accuracy > self.best_acc:\n",
        "        self.update_params(step_size, momentum, deviation)\n",
        "        self.best_acc = accuracy\n",
        "    return self.best_params\n",
        "\n",
        "# Train the model for a single epoch\n",
        "# Returns accuracy and loss\n",
        "def epoch_train(dataloader, model, loss_fn, optimizer, num_of_batches=-1, verbose=False):\n",
        "    model.train()\n",
        "\n",
        "    # Calculate the size of data we run on\n",
        "    if num_of_batches > 0:\n",
        "      size = num_of_batches * BATCH_SIZE\n",
        "    else:\n",
        "      size = TRAIN_SIZE\n",
        "\n",
        "    # loss and accuracy of the entire epoch\n",
        "    epoch_loss, epoch_accuracy  = 0, 0\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "      # Dont run more than num_of_batches batches (-1 means to run all)\n",
        "      if batch == num_of_batches:\n",
        "        break\n",
        "      X, y = X.to(device), y.to(device)\n",
        "\n",
        "      # Compute prediction error\n",
        "      pred = model(X)\n",
        "      loss = loss_fn(pred, y)\n",
        "\n",
        "      # Backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if (batch % 100 == 0) and verbose:\n",
        "          current = (batch + 1) * len(X)\n",
        "          print(f\"Train loss: {loss.item():>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "      epoch_loss += loss.item()\n",
        "      epoch_accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "\n",
        "    # Calculate the total loss and accuracy\n",
        "    if num_of_batches > 0:\n",
        "      loss = epoch_loss / num_of_batches\n",
        "    else:\n",
        "      loss = epoch_loss / len(dataloader)\n",
        "    accuracy = epoch_accuracy / size\n",
        "    return accuracy, loss\n",
        "\n",
        "def test(dataloader, model, loss_fn, num_of_batches=-1, verbose=False):\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch, (X, y) in enumerate(dataloader):\n",
        "          # Dont run more than num_of_batches batches (-1 means to run all)\n",
        "          if batch == num_of_batches:\n",
        "            break\n",
        "          X, y = X.to(device), y.to(device)\n",
        "          pred = model(X)\n",
        "          test_loss += loss_fn(pred, y).item()\n",
        "          correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    # Calculate the total loss and accuracy\n",
        "    if num_of_batches > 0:\n",
        "      size = num_of_batches * BATCH_SIZE\n",
        "      test_loss = test_loss / num_of_batches\n",
        "    else:\n",
        "      size = TEST_SIZE #len(dataloader.dataset)\n",
        "      test_loss = test_loss / len(dataloader)\n",
        "    accuracy = correct / size\n",
        "    if verbose:\n",
        "      print(f\"Test Error: \\n Accuracy: {(100*accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "    return accuracy, test_loss\n",
        "\n",
        "# Run train and test for num_of_epochs\n",
        "# TODO: what should be default for num_of_test_batches?\n",
        "def train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer,\n",
        "                   num_of_train_batches=-1, num_of_test_batches=-1, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False):\n",
        "    all_loss, all_accuracy, all_test_accuracy, all_test_loss = [], [], [], []\n",
        "    for i in range(num_of_epochs):\n",
        "      epoch_accuracy, epoch_loss = epoch_train(train_dataloader, model, loss_fn,\n",
        "                                             optimizer, num_of_batches=num_of_train_batches, verbose=verbose)\n",
        "      test_accuracy, test_loss = test(test_dataloader, model, loss_fn, num_of_batches=num_of_test_batches, verbose=verbose)\n",
        "      all_test_accuracy.append(test_accuracy)\n",
        "      all_test_loss.append(test_loss)\n",
        "      all_accuracy.append(epoch_accuracy)\n",
        "      all_loss.append(epoch_loss)\n",
        "    return all_accuracy, all_loss, all_test_accuracy, all_test_loss\n",
        "\n",
        "# Plot train and test accuracy on the same graph,\n",
        "# and train and test losses on the same graph.\n",
        "def plot_graphs(num_of_epochs, train_accuracy, train_loss, test_accuracy, test_loss, suptitle):\n",
        "  epochs = [(x + 1) for x in range(num_of_epochs)]\n",
        "  fig, axs = plt.subplots(2)\n",
        "  fig.suptitle(suptitle)\n",
        "  fig.tight_layout()#pad=3)\n",
        "  # Plot\n",
        "  #axs[0].set_title(\"Accuracy as func of epochs\")\n",
        "  axs[0].plot(epochs, train_accuracy)\n",
        "  axs[0].plot(epochs, test_accuracy)\n",
        "  axs[0].set(xlabel='epochs', ylabel='accuracy')\n",
        "  axs[0].grid()\n",
        "  axs[0].legend([\"train\", \"test\"], loc=\"best\")\n",
        "\n",
        "  #axs[1].set_title(\"Loss as func of epochs\")\n",
        "  axs[1].plot(epochs, train_loss)\n",
        "  axs[1].plot(epochs, test_loss)\n",
        "  axs[1].set(xlabel='epochs', ylabel='loss')\n",
        "  axs[1].grid()\n",
        "  axs[1].legend([\"train\", \"test\"], loc=\"best\")\n",
        "\n",
        "  plt.show()\n",
        "\n",
        "def pretty_print_results(train_accuracy, train_loss, test_accuracy, test_loss):\n",
        "  # Print the results at the end of optimization\n",
        "  print(f\"Train Error: \\n Accuracy: {(100*train_accuracy[-1]):>0.1f}%, Avg loss: {train_loss[-1]:>8f} \\n\")\n",
        "  print(f\"Test Error: \\n Accuracy: {(100*test_accuracy[-1]):>0.1f}%, Avg loss: {test_loss[-1]:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmDTxh0JcQ1_"
      },
      "source": [
        "### Part 2.1.  Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "sgFMEckAcRD9"
      },
      "outputs": [],
      "source": [
        "# Part 2.1\n",
        "# Run the baseline of this part (train and test) and plot the results\n",
        "# config is a dict of the parameters to run the network with\n",
        "def run_baseline(config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False):\n",
        "  # Train the network with the best parameters and then test it\n",
        "  model = NeuralNetwork().to(device)\n",
        "  model.apply(init_weights_wrapper(config[\"deviation\"]))\n",
        "  # TODO: can we delete this print?\n",
        "  for name, param in model.named_parameters():\n",
        "          print(name , param)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=config[\"step_size\"] , momentum=config[\"momentum\"])\n",
        "  train_accuracy, train_loss, test_accuracy, test_loss =\\\n",
        "          train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer, num_of_epochs=num_of_epochs, verbose=verbose)\n",
        "  \n",
        "  plot_graphs(num_of_epochs, train_accuracy, train_loss, test_accuracy, test_loss, \"2.1 baseline\")\n",
        "\n",
        "  print(\"Baseline results: \")\n",
        "  pretty_print_results(train_accuracy, train_loss, test_accuracy, test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_Kl3ZgFcvSH"
      },
      "source": [
        "### Part 2.2. Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Ll9IvLqjyCoU"
      },
      "outputs": [],
      "source": [
        "# Part 2.2\n",
        "def change_optimization(config, train_dataloader, test_dataloader, betas, num_of_epochs=60, verbose=False, cnn=False):\n",
        "  if cnn:\n",
        "    model = CNN().to(device)\n",
        "  else:\n",
        "    model = NeuralNetwork().to(device)\n",
        "  model.apply(init_weights_wrapper(config[\"deviation\"]))\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.Adam(model.parameters(), lr=config[\"step_size\"] , betas=betas)\n",
        "  train_accuracy, train_loss, test_accuracy, test_loss =\\\n",
        "          train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer, num_of_epochs=num_of_epochs, verbose=verbose)\n",
        "  \n",
        "  plot_graphs(num_of_epochs, train_accuracy, train_loss, test_accuracy, test_loss, \"2.2 optimization\")\n",
        "  \n",
        "  print(\"Adam optimization results for: \" + str(betas))\n",
        "  pretty_print_results(train_accuracy, train_loss, test_accuracy, test_loss)\n",
        "\n",
        "def beta_gs(config, train_dataloader, test_dataloader, num_of_epochs=20, verbose=False):\n",
        "  for b1 in [0.9 , 0.8 , 0.7 , 0.6 , 0.5]:\n",
        "    for b2 in [0.999 ,  0.888 , 0.777 , 0.666 , 0.555]:\n",
        "      change_optimization(config, train_dataloader, test_dataloader, num_of_epochs=num_of_epochs , betas=(b1 , b2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6wUKu2G3c0Z1"
      },
      "source": [
        "### Part 2.3. Initialization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2iXoNz8B7eVZ"
      },
      "outputs": [],
      "source": [
        "# Part 2.3\n",
        "\n",
        "def init_weights_with_xavier(m):\n",
        "    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n",
        "        torch.nn.init.xavier_uniform_(m.weight)\n",
        "        #torch.nn.init.xavier_uniform_(m.bias)\n",
        "        m.bias.data.fill_(0)\n",
        "\n",
        "def change_initialization(config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False , cnn=False):\n",
        "  if cnn:\n",
        "    model = CNN().to(device)\n",
        "  else:\n",
        "    model = NeuralNetwork().to(device)\n",
        "  model.apply(init_weights_with_xavier)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=config[\"step_size\"] , momentum=config[\"momentum\"])\n",
        "  train_accuracy, train_loss, test_accuracy, test_loss =\\\n",
        "          train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer, num_of_epochs=num_of_epochs, verbose=verbose)\n",
        "  \n",
        "  plot_graphs(num_of_epochs, train_accuracy, train_loss, test_accuracy, test_loss, \"2.3 initialization\")\n",
        "  \n",
        "  print(\"Xavier initialization results: \")\n",
        "  pretty_print_results(train_accuracy, train_loss, test_accuracy, test_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a2rNE09fc3dX"
      },
      "source": [
        "### Part 2.4. Regularization\n",
        "Add dropout and weight decay to the network"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "_iffBxcw-h4s"
      },
      "outputs": [],
      "source": [
        "# Part 2.4\n",
        "\n",
        "class DropoutNeuralNetwork(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(3*32*32, 256),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "            #nn.Dropout(dropout)\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def regularization(config, train_dataloader, test_dataloader, dropout=0.05, weight_decay=0.1, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False, cnn=False):\n",
        "  if cnn:\n",
        "    model = CNNWithDO(dropout).to(device)\n",
        "  else:\n",
        "    model = DropoutNeuralNetwork(dropout).to(device)\n",
        "  model.apply(init_weights_wrapper(config[\"deviation\"]))\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=config[\"step_size\"] , momentum=config[\"momentum\"] , weight_decay=weight_decay)\n",
        "  train_accuracy, train_loss, test_accuracy, test_loss =\\\n",
        "          train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer, num_of_epochs=num_of_epochs, verbose=verbose)\n",
        "  \n",
        "  plot_graphs(num_of_epochs, train_accuracy, train_loss, test_accuracy, test_loss, \"2.4 regularization\")\n",
        "\n",
        "  print(\"Regularization results: \")\n",
        "  pretty_print_results(train_accuracy, train_loss, test_accuracy, test_loss)\n",
        "  print(\"Weight decay is: \", weight_decay, \" dropout is: \", dropout)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mwK0NFD6dQ2L"
      },
      "source": [
        "### Part 2.5. Preprocessing\n",
        "Add PCA whitening"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "O2rbE4RyR3Af"
      },
      "outputs": [],
      "source": [
        "# Part 2.5\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "#torch.cat([x[0] for x in training_data])\n",
        "\n",
        "class NeuralNetworkWithPreprocess(nn.Module):\n",
        "    def __init__(self, pca):\n",
        "        super().__init__()\n",
        "        self.pca = pca\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(BATCH_SIZE, 256),\n",
        "            #nn.Linear(1, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def whitening(self, x):\n",
        "        x_ = self.flatten(x)\n",
        "        x_ = self.pca.transform(torch.Tensor.cpu(x_))\n",
        "        x = (torch.tensor(x_)).to(device).to(torch.float32)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply preprocessing        \n",
        "        x = self.whitening(x)\n",
        "        # Apply the network\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def get_pca(train_sampler):\n",
        "  train_dataloader = DataLoader(training_data, batch_size=TRAIN_SIZE, sampler=train_sampler)\n",
        "  pca = PCA(whiten=True, n_components=64) # TODO: not sure about n_comp\n",
        "  x, y = next(iter(train_dataloader))\n",
        "  x = x.reshape((TRAIN_SIZE, -1))\n",
        "  pca.fit(x)\n",
        "  return pca\n",
        "\n",
        "def add_preprocessing(config, train_sampler, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False, cnn=False):\n",
        "  pca = get_pca(train_sampler)\n",
        "  if cnn:\n",
        "    model = CnnWithPreprocess(pca).to(device)\n",
        "  else:\n",
        "    model = NeuralNetworkWithPreprocess(pca).to(device)\n",
        "  model.apply(init_weights_wrapper(config[\"deviation\"]))\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=config[\"step_size\"] , momentum=config[\"momentum\"])\n",
        "  train_accuracy, train_loss, test_accuracy, test_loss =\\\n",
        "          train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer, num_of_epochs=num_of_epochs, verbose=verbose)\n",
        "  plot_graphs(num_of_epochs, train_accuracy, train_loss, test_accuracy, test_loss, \"2.5 preprocessing\")\n",
        "\n",
        "  print(\"Preprocessing results: \")\n",
        "  pretty_print_results(train_accuracy, train_loss, test_accuracy, test_loss)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBqjllAudYT-"
      },
      "source": [
        "### Part 2.6. Network width"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "jLO2rhgeow7s"
      },
      "outputs": [],
      "source": [
        "# Part 2.6\n",
        "\n",
        "class NeuralNetworkChangingWidth(nn.Module):\n",
        "    def __init__(self, width):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(3*32*32, width),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(width, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "# Create a fig for plotting train and test accuracy and loss\n",
        "def prepare_fig(suptitle):\n",
        "  fig, axs = plt.subplots(2)\n",
        "  fig.suptitle(suptitle)\n",
        "  fig.tight_layout()#pad=3)\n",
        "  axs[0].set(xlabel='epochs', ylabel='accuracy')\n",
        "  axs[0].grid()\n",
        "  #axs[0].legend(legend, loc=\"best\")\n",
        "  axs[1].set(xlabel='epochs', ylabel='loss')\n",
        "  axs[1].grid()\n",
        "  #axs[1].legend(legend, loc=\"best\")\n",
        "  return fig, axs  \n",
        "\n",
        "def plot(axs, num_of_epochs, train_accuracy,\n",
        "         train_loss, test_accuracy, test_loss):\n",
        "  # Plot\n",
        "  epochs = [(x + 1) for x in range(num_of_epochs)]\n",
        "  axs[0].plot(epochs, train_accuracy)\n",
        "  axs[0].plot(epochs, test_accuracy)\n",
        "  axs[1].plot(epochs, train_loss)\n",
        "  axs[1].plot(epochs, test_loss)\n",
        "  \n",
        "\n",
        "def change_width(config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False , cnn=False):\n",
        "  fig, axs = prepare_fig(\"2.6 width\")\n",
        "  # TODO: If we can move  legend a little bit to the side\n",
        "  legend = []\n",
        "  if cnn:\n",
        "    values = [(256 , 64) , (512 , 256)]\n",
        "  else:\n",
        "    values = [2**6, 2**10, 2**12]\n",
        "  for i in values:\n",
        "    width =  i\n",
        "    if cnn:\n",
        "      model = CNNChangingWidth(width).to(device)\n",
        "    else:\n",
        "      model = NeuralNetworkChangingWidth(width).to(device)\n",
        "    \n",
        "    \n",
        "    model.apply(init_weights_wrapper(config[\"deviation\"]))\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=config[\"step_size\"] , momentum=config[\"momentum\"])\n",
        "    train_accuracy, train_loss, test_accuracy, test_loss =\\\n",
        "            train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer, num_of_epochs=num_of_epochs, verbose=verbose)\n",
        "  \n",
        "    plot(axs, num_of_epochs, train_accuracy, train_loss, test_accuracy, test_loss)\n",
        "    legend.append(\"train width {}\".format(width))\n",
        "    legend.append(\"test width {}\".format(width))\n",
        "\n",
        "    print(\"Changing width results, with width\", width, \":\")\n",
        "    pretty_print_results(train_accuracy, train_loss, test_accuracy, test_loss)\n",
        "  fig.legend(legend, loc=\"right\")\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HKuftmldb6t"
      },
      "source": [
        "### Part 2.7. Network Depth"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# part 2.7\n",
        "\n",
        "class NeuralNetworkChangingDepth(nn.Module):\n",
        "    def __init__(self, depth): # width set to 64 as requested and depth is adjustable\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.stack = nn.ModuleList([nn.Linear(3*32*32, 64) , nn.ReLU()])\n",
        "        #self.stack.append(nn.Linear(3*32*32, 64))\n",
        "        for i in range(depth - 2):\n",
        "            #output_size = layer_sizes[i+1]\n",
        "            self.stack.append(nn.Linear(64, 64))\n",
        "            self.stack.append(nn.ReLU())\n",
        "            \n",
        "            \n",
        "        self.stack.append(nn.Linear(64, 10))\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.flatten(x)\n",
        "      for layer in self.stack: #enumerate(self.stack):\n",
        "          #temp = self.stack[i//2#](x)\n",
        "          x = layer(x)\n",
        "      return x\n",
        "      # for layer in self.stack[:-1]:\n",
        "\n",
        "      #       x = layer(x).clamp(min=0)\n",
        "      # return self.stack[-1](x)\n",
        "\n",
        "def try_with_different_depth(config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False):\n",
        "  \n",
        "  # TODO: If we can move  legend a little bit to the side\n",
        "  num_of_epochs = 100\n",
        "  for depth in [3, 4 , 10]: \n",
        "    fig, axs = prepare_fig(\"2.7 depth \" + str(depth))\n",
        "    legend = []\n",
        "    model = NeuralNetworkChangingDepth(depth).to(device)\n",
        "    model.apply(init_weights_wrapper(config[\"deviation\"]))\n",
        "\n",
        "    for name, param in model.named_parameters():\n",
        "          print(name , param)\n",
        "\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.SGD(model.parameters(), lr=config[\"step_size\"] , momentum=config[\"momentum\"])\n",
        "    train_accuracy, train_loss, test_accuracy, test_loss =\\\n",
        "            train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer, num_of_epochs=num_of_epochs, verbose=verbose)\n",
        "  \n",
        "    plot(axs, num_of_epochs, train_accuracy, train_loss, test_accuracy, test_loss)\n",
        "    legend.append(\"train depth {}\".format(depth))\n",
        "    legend.append(\"test depth {}\".format(depth))\n",
        "\n",
        "    print(\"Changing depth results, with depth\", depth, \":\")\n",
        "    pretty_print_results(train_accuracy, train_loss, test_accuracy, test_loss)\n",
        "    fig.legend(legend, loc=\"right\")\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "Y42ZWE7qEoyu"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUaBud2LvE36"
      },
      "source": [
        "#Question 3 - Convolutional Neural Network\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "IDL8RtZEvYkE"
      },
      "outputs": [],
      "source": [
        "CNNconfig = {\n",
        "    \"step_size\": [5e-4, 7e-4,  1e-3, 1e-2],\n",
        "    \"momentum\": [0.45, 0.5, 0.55, 0.7, 0.8],\n",
        "    \"deviation\": [1, 1.5, 2, 2.25, 2.5,  3]\n",
        "}\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16*6*6, 784),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(784, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWCQtbBcvQI-"
      },
      "source": [
        "### Part 3.1. Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "sX3m-SlI0cXw"
      },
      "outputs": [],
      "source": [
        "# Part 3.1\n",
        "\n",
        "def cnn_run_baseline(config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False):\n",
        "  model = CNN().to(device)\n",
        "  model.apply(init_weights_wrapper(config[\"deviation\"]))\n",
        "  # TODO: can we delete this print?\n",
        "  #for name, param in model.named_parameters():\n",
        "  #        print(name , param)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=config[\"step_size\"] , momentum=config[\"momentum\"])\n",
        "  train_accuracy, train_loss, test_accuracy, test_loss =\\\n",
        "          train_and_test(train_dataloader, test_dataloader, model, loss_fn, optimizer, num_of_epochs=num_of_epochs, verbose=verbose)\n",
        "  \n",
        "  plot_graphs(num_of_epochs, train_accuracy, train_loss, test_accuracy, test_loss, \"3.1 baseline\")\n",
        "\n",
        "  print(\"Baseline results: \")\n",
        "  pretty_print_results(train_accuracy, train_loss, test_accuracy, test_loss )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 3.2 - Optimization\n"
      ],
      "metadata": {
        "id": "F_VfYv32t4X4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 3.2 - optimization\n",
        "def cnn_change_optimization(config, train_dataloader, test_dataloader, betas, num_of_epochs=60, verbose=False):\n",
        "  change_optimization(config, train_dataloader, test_dataloader, betas, num_of_epochs=num_of_epochs, verbose=verbose, cnn=True)"
      ],
      "metadata": {
        "id": "u2TAAlgzPAIQ"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 3.3 - Initialization"
      ],
      "metadata": {
        "id": "04dcGwiQy2xY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#part 3.3 init\n",
        "def cnn_with_initialization(config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False):\n",
        "  change_initialization(config, train_dataloader, test_dataloader, num_of_epochs=num_of_epochs, verbose=verbose, cnn=True)\n"
      ],
      "metadata": {
        "id": "mBcP2uIyzBcM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 3.4 - Regularization"
      ],
      "metadata": {
        "id": "yHuxlFCR2Hkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 3.3 - regularizaton\n",
        "class CNNWithDO(nn.Module):\n",
        "    def __init__(self, dropout):\n",
        "        super().__init__()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16*6*6, 784),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(784, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def cnn_with_regularization(config, train_dataloader, test_dataloader, dropout=0.05, weight_decay=0.1, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False):\n",
        "  regularization(config, train_dataloader, test_dataloader, dropout=dropout, weight_decay=weight_decay, num_of_epochs=num_of_epochs, verbose=verbose, cnn=True)"
      ],
      "metadata": {
        "id": "3KfvIr3R2HJX"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 3.5 - Preprocessing"
      ],
      "metadata": {
        "id": "zXFlNEamAGAz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CnnWithPreprocess(nn.Module):\n",
        "    def __init__(self, pca):\n",
        "        super().__init__()\n",
        "        self.pca = pca\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16*6*6, 784),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(784, 10)\n",
        "        )\n",
        "        \n",
        "    def whitening(self, x):\n",
        "        x_ = self.flatten(x)\n",
        "        x_ = self.pca.transform(torch.Tensor.cpu(x_))\n",
        "        x = (torch.tensor(x_)).to(device).to(torch.float32)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply preprocessing        \n",
        "        x = self.whitening(x)\n",
        "        # Apply the network\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "def cnn_add_preprocessing(config, train_sampler, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False):\n",
        "  add_preprocessing(config, train_sampler, train_dataloader, test_dataloader, num_of_epochs=num_of_epochs, verbose=verbose, cnn=True)"
      ],
      "metadata": {
        "id": "gBcG1zMQANGG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 3.6 - Width"
      ],
      "metadata": {
        "id": "L7ynBSAeCrlR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#part 3.6 - width\n",
        "\n",
        "class CNNChangingWidth(nn.Module):\n",
        "    def __init__(self , filter_sizes):\n",
        "        super().__init__()\n",
        "        self.filter_sizes = filter_sizes\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=filter_sizes[0], kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=filter_sizes[0], out_channels=filter_sizes[1], kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(filter_sizes[1]*6*6, 784),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(784, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "def cnn_change_width(config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False):\n",
        "  change_width(config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False , cnn=True) #use same function as in 2.6\n"
      ],
      "metadata": {
        "id": "Tg56QyXMDC57"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Part 3.7 - Depth"
      ],
      "metadata": {
        "id": "fOTW29sFIPPu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#part 3.7 - depth\n",
        "\n",
        "class CNNChangingDepth(nn.Module):\n",
        "    def __init__(self, depth):\n",
        "        super().__init__()\n",
        "        # 3 * 32 * 32 ->(conv) 64*30*30 -> 64 * 15 * 15\n",
        "        self.stack = nn.ModuleList([\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2)])\n",
        "        for i in range(depth - 2):\n",
        "            self.stack.extend(\n",
        "                [nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=\"same\"),\n",
        "                 nn.ReLU(),\n",
        "                 nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            )\n",
        "        # 64 * (15// (2 ** depth)) * (15 // (2 ** depth))\n",
        "        size = ((15 // (2 ** (depth - 2))) - 2) // 2\n",
        "        self.stack.extend([\n",
        "            nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16 * size * size, 784),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(784, 10)])\n",
        "\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Conv2d(in_channels=64, out_channels=16, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(16*6*6, 784),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(784, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        #x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "class NeuralNetworkChangingDepth(nn.Module):\n",
        "    def __init__(self, depth): # width set to 64 as requested and depth is adjustable\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.stack = nn.ModuleList([nn.Linear(3*32*32, 64) , nn.ReLU()])\n",
        "        #self.stack.append(nn.Linear(3*32*32, 64))\n",
        "        for i in range(depth - 2):\n",
        "            #output_size = layer_sizes[i+1]\n",
        "            self.stack.append(nn.Linear(64, 64))\n",
        "            self.stack.append(nn.ReLU())\n",
        "            \n",
        "            \n",
        "        self.stack.append(nn.Linear(64, 10))\n",
        "        \n",
        "\n",
        "    def forward(self, x):\n",
        "      x = self.flatten(x)\n",
        "      for layer in self.stack: #enumerate(self.stack):\n",
        "          #temp = self.stack[i//2#](x)\n",
        "          x = layer(x)\n",
        "      return x\n",
        "\n",
        "def cnn_try_with_different_depth():\n",
        "  pass\n",
        "  # TODO: to write"
      ],
      "metadata": {
        "id": "JBp4h2PMIOtW"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWGZqYkxdfq5"
      },
      "source": [
        "# Main\n",
        "Run all the code of the exercise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "-f0Q1avTBKrI"
      },
      "outputs": [],
      "source": [
        "# Call all the functions of question 2\n",
        "def Q2(config, train_dataloader, test_dataloader):\n",
        "  # Choose which parts to run\n",
        "  should_run_gs = False\n",
        "  should_run_baseline = False\n",
        "  should_run_change_optimization = False\n",
        "  should_run_change_initialization = False\n",
        "  should_run_regularization = False\n",
        "  should_run_preprocessing = False\n",
        "  should_run_width = False\n",
        "  should_run_depth = False\n",
        "  \n",
        "  # Choose best parameters\n",
        "  #TODO: run grid search with zero biases\n",
        "  best_config = {}\n",
        "  if should_run_gs:\n",
        "    gs = GridSearch(\"grid_search_q2\")\n",
        "    best_config = gs.run(train_dataloader, test_dataloader, config, verbose=False)\n",
        "    #best_config[\"step_size\"] = gs.best_params[\"step_size\"]\n",
        "    #best_config[\"momentum\"] = gs.best_params[\"momentum\"]\n",
        "    #best_config[\"deviation\"] =  gs.best_params[\"deviation\"]\n",
        "    files.download('grid_search_q2') # TODO: shuold delete?\n",
        "    print(\"Best config is: \")\n",
        "    print(best_config)\n",
        "  else:\n",
        "    \n",
        "    # Best\n",
        "    best_config[\"step_size\"] = 0.01\n",
        "    best_config[\"momentum\"] = 0.8\n",
        "    best_config[\"deviation\"] =  0.01\n",
        "    \n",
        "    \n",
        "\n",
        "  if should_run_baseline:\n",
        "    print(\"Run baseline\")\n",
        "    run_baseline(best_config, train_dataloader, test_dataloader, verbose=False)\n",
        "    print()\n",
        "  if should_run_change_optimization:\n",
        "    print(\"Run change optimization\")\n",
        "    # Change step size for optimization\n",
        "    best_config[\"step_size\"] = 0.001\n",
        "    change_optimization(best_config, train_dataloader, test_dataloader, verbose=False , betas=(0.7, 0.555))\n",
        "    best_config[\"step_size\"] = 0.01\n",
        "    print()\n",
        "  if should_run_change_initialization:\n",
        "    print(\"Run change initialization\")\n",
        "    change_initialization(best_config, train_dataloader, test_dataloader, verbose=False)\n",
        "    print()\n",
        "  if should_run_regularization:\n",
        "    print(\"Run regularization\")\n",
        "    regularization(best_config, train_dataloader, test_dataloader, num_of_epochs=100, verbose=False)\n",
        "    print()\n",
        "  if should_run_preprocessing:\n",
        "    print(\"run preprocessing\")\n",
        "    add_preprocessing(best_config, train_sampler, train_dataloader, test_dataloader, verbose=False)\n",
        "    print()\n",
        "  if should_run_width:\n",
        "    print(\"Run network width\")\n",
        "    change_width(best_config, train_dataloader, test_dataloader, verbose=False)\n",
        "    print()\n",
        "  if should_run_depth:\n",
        "    print(\"Run network width\")\n",
        "    try_with_different_depth(best_config, train_dataloader, test_dataloader, verbose=False)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "0NUCcVyHCAzl"
      },
      "outputs": [],
      "source": [
        "# Call all the functions of question 2\n",
        "def Q3(config, train_dataloader, test_dataloader):\n",
        "  should_run_gs = False\n",
        "  should_run_baseline = False\n",
        "  should_run_change_optimization = False\n",
        "  should_run_change_initialization = False\n",
        "  should_run_regularization = False\n",
        "  should_run_preprocessing = False\n",
        "  should_run_width = True\n",
        "  should_run_depth = False\n",
        "\n",
        "  best_config = {}\n",
        "  if should_run_gs:\n",
        "    gs = GridSearch(\"grid_search_q3\")\n",
        "    best_config = gs.run(train_dataloader, test_dataloader, config, verbose=False)\n",
        "    #best_config[\"step_size\"] = gs.best_params[\"step_size\"]\n",
        "    #best_config[\"momentum\"] = gs.best_params[\"momentum\"]\n",
        "    #best_config[\"deviation\"] =  gs.best_params[\"deviation\"]\n",
        "    files.download('grid_search_q3')\n",
        "    print(\"Best config is: \")\n",
        "    print(best_config)\n",
        "  else:\n",
        "    # The best config is the result of previous run of grid search\n",
        "    best_config[\"step_size\"] = 0.01\n",
        "    best_config[\"momentum\"] = 0.8\n",
        "    best_config[\"deviation\"] =  0.1\n",
        "  if should_run_baseline:\n",
        "    print(\"Run baseline\")\n",
        "    cnn_run_baseline(best_config, train_dataloader, test_dataloader, verbose=False , num_of_epochs=60) #TODO: change to 100\n",
        "    print()\n",
        "  if should_run_change_optimization:\n",
        "    print(\"Run change optimization\")\n",
        "    # Change step size for optimization\n",
        "    # TODO: should gs the betas\n",
        "    best_config[\"step_size\"] = 0.001\n",
        "    cnn_change_optimization(best_config, train_dataloader, test_dataloader, verbose=False , betas=(0.7, 0.555))\n",
        "    best_config[\"step_size\"] = 0.01\n",
        "    print()\n",
        "  if should_run_change_initialization:\n",
        "    print(\"Run change initialization\")\n",
        "    cnn_with_initialization(best_config, train_dataloader, test_dataloader, verbose=False)\n",
        "    print()\n",
        "  if should_run_regularization:\n",
        "    print(\"Run regularization\")\n",
        "    cnn_with_regularization(best_config, train_dataloader, test_dataloader, num_of_epochs=100, verbose=False)\n",
        "    print()\n",
        "  if should_run_preprocessing:\n",
        "    print(\"run preprocessing\")\n",
        "    cnn_add_preprocessing(best_config, train_sampler, train_dataloader, test_dataloader, verbose=False)\n",
        "    print()\n",
        "  if should_run_width:\n",
        "    print(\"Run network width\")\n",
        "    cnn_change_width(best_config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False)\n",
        "    print()\n",
        "  if should_run_depth:\n",
        "    print(\"Run network width\")\n",
        "    cnn_try_with_different_depth(best_config, train_dataloader, test_dataloader, num_of_epochs=DEFAULT_NUM_OF_EPOCHS, verbose=False)\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TmomOSl3Tx28",
        "outputId": "ce22d593-786a-4d3f-bea1-474b28abba35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run network width\n",
            "Changing width results, with width (256, 64) :\n",
            "Train Error: \n",
            " Accuracy: 9.5%, Avg loss: 2.303068 \n",
            "\n",
            "Test Error: \n",
            " Accuracy: 10.7%, Avg loss: 2.302239 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "should_run_svm = False\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  # Question 1\n",
        "  train_sampler = torch.utils.data.RandomSampler(training_data, replacement=True, num_samples=TRAIN_SIZE, generator=None)\n",
        "  test_sampler = torch.utils.data.RandomSampler(test_data, replacement=True, num_samples=TEST_SIZE, generator=None)\n",
        "\n",
        "  if should_run_svm:\n",
        "    svm(train_sampler, test_sampler)\n",
        "\n",
        "  # Loaders for NN\n",
        "  num_workers = 1\n",
        "  train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE, sampler=train_sampler , num_workers=num_workers)\n",
        "  test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, sampler=test_sampler , num_workers=num_workers)\n",
        "\n",
        "  # TODO: should choose parameters here\n",
        "  config = {\n",
        "    \"step_size\": [5e-4, 1e-3, 1e-2, 1e-1],\n",
        "    \"momentum\": [0.2, 0.5, 0.8, 0.9],\n",
        "    \"deviation\": [1e-2,1e-1, 1, 2, 2.5]\n",
        "  }\n",
        "\n",
        "  Q2(config, train_dataloader, test_dataloader)\n",
        "  Q3(config, train_dataloader, test_dataloader)\n",
        "\n",
        "  print(\"end\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vNGqDwd4r21j"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}