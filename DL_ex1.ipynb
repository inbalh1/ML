{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/inbalh1/ML/blob/main/DL_ex1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zqy_C_lHYIBg",
        "outputId": "ac9f65e1-3ef1-4986-d985-08fac5291483"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n",
            "0.001\n",
            "0.5\n",
            "2.5\n",
            "linear_relu_stack.0.weight Parameter containing:\n",
            "tensor([[-0.4040,  4.1538,  3.9677,  ...,  2.1561,  5.7466, -1.8322],\n",
            "        [-0.8679, -2.0312,  3.2370,  ..., -1.1132, -0.3474, -0.0911],\n",
            "        [ 1.8236, -4.3124, -3.6771,  ..., -1.6617,  1.6285,  2.1214],\n",
            "        ...,\n",
            "        [ 0.8944, -0.1653, -2.3597,  ...,  0.3337,  2.6698, -1.2430],\n",
            "        [-0.1854,  3.5965,  2.6563,  ...,  0.3198, -1.6843,  1.6330],\n",
            "        [-1.6915,  3.5048, -1.9217,  ..., -0.7926, -1.9742,  2.1874]],\n",
            "       device='cuda:0', requires_grad=True)\n",
            "linear_relu_stack.0.bias Parameter containing:\n",
            "tensor([-3.7531e+00,  2.8275e+00, -4.6132e+00,  4.5471e-01,  1.8423e-01,\n",
            "        -1.2174e+00, -4.0026e-01,  1.5917e+00, -4.0583e+00,  1.4927e+00,\n",
            "        -4.6483e-03,  3.1037e+00, -2.9726e+00, -1.5423e+00, -1.7654e+00,\n",
            "         6.2449e-01,  2.4461e+00, -3.4186e+00, -5.7997e-01,  2.6657e+00,\n",
            "        -2.6111e+00,  2.0900e+00,  1.7462e+00,  2.5212e-02, -3.3310e+00,\n",
            "         3.3319e+00, -5.1766e+00, -1.4219e+00,  1.8406e+00,  2.1755e+00,\n",
            "         8.7288e-01, -2.2430e+00,  1.8707e+00, -1.5495e+00,  1.9844e+00,\n",
            "        -3.1214e+00, -3.3537e+00,  3.6211e-02,  1.9433e+00, -3.1885e+00,\n",
            "        -1.1049e+00,  1.4074e+00, -1.7258e+00,  6.8134e-02, -7.7094e-02,\n",
            "         2.5288e+00, -3.3515e+00,  3.7195e+00, -7.8316e-01,  9.7072e-01,\n",
            "        -6.6922e-01, -7.1789e+00, -9.1106e-01, -4.1723e+00, -3.7788e+00,\n",
            "         2.8845e-01, -2.7778e+00,  2.8579e+00,  2.0498e+00, -2.3785e+00,\n",
            "         7.6414e-01, -3.7437e+00, -1.3922e+00,  9.8815e-01,  6.7621e-01,\n",
            "         1.7312e+00,  1.7612e+00, -7.5396e-01,  9.0988e-01,  7.0801e-01,\n",
            "         1.5370e+00,  1.0769e+00,  1.2579e-01,  2.6939e+00, -2.4403e+00,\n",
            "        -2.2574e+00, -1.4390e+00, -1.6105e+00, -2.3123e+00,  2.8513e+00,\n",
            "        -2.8536e+00, -8.9832e-02,  2.6899e+00,  8.1559e-01,  7.5644e+00,\n",
            "        -3.8993e+00,  2.6764e+00, -4.9174e+00, -2.2044e+00, -6.1813e-02,\n",
            "         4.6684e-01,  3.4363e+00, -3.7493e+00, -1.6035e+00, -3.7737e+00,\n",
            "         2.7601e+00,  3.7024e+00,  2.2421e+00, -1.6752e+00, -9.7614e-01,\n",
            "         2.0950e+00,  2.1993e+00,  6.4882e+00,  3.0212e+00,  2.3966e+00,\n",
            "         2.7913e+00, -2.4124e-02, -7.1866e+00,  8.3555e-01, -3.0281e+00,\n",
            "        -4.3438e-01,  9.3282e-01, -1.1217e-01, -3.6921e+00, -6.0060e-01,\n",
            "        -1.5151e+00, -2.0375e+00, -5.8081e-01,  5.5924e-01,  3.1230e+00,\n",
            "         4.9337e-01, -4.9130e+00, -2.6113e+00,  3.8070e+00, -1.6392e+00,\n",
            "        -3.5603e-01,  7.7136e-02, -3.2313e+00,  1.9263e+00, -6.0608e+00,\n",
            "         8.8107e-01,  2.1342e+00,  2.1500e+00,  6.0615e-01, -2.3036e+00,\n",
            "         6.6388e+00,  2.0285e-01,  1.5466e+00,  2.0292e+00,  1.4216e+00,\n",
            "        -5.9141e-01, -3.3770e+00,  1.5600e+00,  4.9886e-02, -2.3215e+00,\n",
            "        -1.3719e+00, -5.2021e+00, -1.8867e+00, -1.5072e+00,  1.6955e+00,\n",
            "         2.5267e-01,  1.6384e-01, -7.8722e-01,  5.1424e+00, -7.2365e-01,\n",
            "         7.0050e+00, -1.6386e+00,  2.3253e+00, -2.8525e-01,  1.0412e+00,\n",
            "         2.8952e+00, -1.3291e+00, -4.4559e-01, -2.4737e+00, -2.2785e+00,\n",
            "        -1.7856e+00,  2.5025e+00, -7.5870e-02,  4.4366e+00,  1.4358e+00,\n",
            "        -1.0233e+00, -1.0547e+00, -3.7614e-01,  2.0333e+00,  2.3398e-01,\n",
            "         1.9644e+00, -1.6446e+00, -1.9516e+00,  2.2271e+00, -4.6538e+00,\n",
            "        -2.0949e+00,  1.4043e+00, -1.3415e+00, -2.7703e+00, -1.3775e+00,\n",
            "        -4.6738e+00,  2.8122e+00, -5.0412e-01,  1.4893e+00,  3.0629e+00,\n",
            "        -7.3486e-01, -5.3507e-01,  5.3061e-01, -8.9429e-02,  4.9072e-01,\n",
            "         8.4459e-01, -9.4662e-01,  8.5556e-01,  2.2780e-02,  3.6664e+00,\n",
            "        -4.0709e+00,  1.4951e-01, -3.2414e+00, -1.3681e+00,  2.0334e+00,\n",
            "         1.5252e+00, -1.7153e-01, -2.8588e+00, -1.1423e+00,  1.1518e+00,\n",
            "         8.8895e-01,  1.5435e+00, -3.1272e+00,  1.1931e+00, -2.4149e+00,\n",
            "         1.5389e+00,  2.2565e+00, -1.5068e+00,  1.8080e+00, -5.6397e-01,\n",
            "         3.8191e+00, -1.2268e+00,  4.3490e+00,  2.9910e+00,  9.3209e-01,\n",
            "        -1.3110e+00, -2.7175e-01, -1.2008e+00, -8.8405e-01,  3.0320e+00,\n",
            "         1.7848e+00,  5.4014e-01, -3.0945e-01, -1.5563e+00, -1.0136e+00,\n",
            "         1.4440e-01,  1.1734e+00,  5.8652e+00, -8.1319e-02, -3.6613e+00,\n",
            "         3.6447e+00, -2.3252e+00, -4.7234e-01,  1.9760e+00, -3.5450e+00,\n",
            "        -2.9199e-01, -1.2122e+00, -1.8439e+00,  1.6700e+00, -4.1873e+00,\n",
            "        -3.0873e+00,  4.8006e+00, -2.0542e+00, -7.7726e-01, -5.2208e+00,\n",
            "         2.2627e+00], device='cuda:0', requires_grad=True)\n",
            "linear_relu_stack.2.weight Parameter containing:\n",
            "tensor([[-4.3124e-01,  6.5613e-01, -5.3339e-01,  ..., -7.0043e-01,\n",
            "         -1.5158e+00,  2.0390e+00],\n",
            "        [-6.0986e-01,  2.2005e+00, -2.5740e-01,  ...,  1.0195e-01,\n",
            "         -1.3031e+00,  5.3790e-03],\n",
            "        [ 2.0410e+00, -2.5008e-01,  5.1265e-01,  ..., -1.1681e+00,\n",
            "         -1.8904e+00,  5.0783e+00],\n",
            "        ...,\n",
            "        [ 1.5264e+00, -1.6627e+00,  2.6132e+00,  ..., -1.7422e+00,\n",
            "          7.2152e+00,  1.5903e+00],\n",
            "        [-2.2603e+00, -4.9075e-01, -4.6802e+00,  ..., -1.1817e+00,\n",
            "          4.8832e+00, -7.0442e-01],\n",
            "        [ 2.9729e+00,  4.0812e-01,  1.7581e+00,  ..., -1.0926e+00,\n",
            "          8.6370e-01,  1.9626e+00]], device='cuda:0', requires_grad=True)\n",
            "linear_relu_stack.2.bias Parameter containing:\n",
            "tensor([-1.5174, -0.8134, -1.2702,  0.7249,  0.1312, -3.3853, -1.9564,  4.2065,\n",
            "         3.4730, -1.9058], device='cuda:0', requires_grad=True)\n",
            "train loss: 6184.697754  [   64/ 5000]\n",
            "train loss: 806.877930  [   64/ 5000]\n",
            "train loss: 544.539978  [   64/ 5000]\n",
            "train loss: 783.377747  [   64/ 5000]\n",
            "train loss: 518.324768  [   64/ 5000]\n",
            "train loss: 619.169434  [   64/ 5000]\n",
            "train loss: 479.097168  [   64/ 5000]\n",
            "train loss: 479.198578  [   64/ 5000]\n",
            "train loss: 407.831146  [   64/ 5000]\n",
            "train loss: 507.468262  [   64/ 5000]\n",
            "train loss: 426.364807  [   64/ 5000]\n",
            "train loss: 372.669464  [   64/ 5000]\n",
            "train loss: 392.607788  [   64/ 5000]\n",
            "train loss: 355.161285  [   64/ 5000]\n",
            "train loss: 347.463226  [   64/ 5000]\n",
            "train loss: 431.446442  [   64/ 5000]\n",
            "train loss: 297.266663  [   64/ 5000]\n",
            "train loss: 326.660583  [   64/ 5000]\n",
            "train loss: 324.814911  [   64/ 5000]\n",
            "train loss: 245.381073  [   64/ 5000]\n",
            "train loss: 251.098602  [   64/ 5000]\n",
            "train loss: 258.212952  [   64/ 5000]\n",
            "train loss: 188.566574  [   64/ 5000]\n",
            "train loss: 273.220306  [   64/ 5000]\n",
            "train loss: 246.861542  [   64/ 5000]\n",
            "train loss: 245.146561  [   64/ 5000]\n",
            "train loss: 238.678101  [   64/ 5000]\n",
            "train loss: 292.950806  [   64/ 5000]\n",
            "train loss: 214.351852  [   64/ 5000]\n",
            "train loss: 217.709625  [   64/ 5000]\n",
            "train loss: 175.691254  [   64/ 5000]\n",
            "train loss: 233.786133  [   64/ 5000]\n",
            "train loss: 158.048813  [   64/ 5000]\n",
            "train loss: 233.862427  [   64/ 5000]\n",
            "train loss: 163.576492  [   64/ 5000]\n",
            "train loss: 165.497269  [   64/ 5000]\n",
            "train loss: 191.974045  [   64/ 5000]\n",
            "train loss: 208.407883  [   64/ 5000]\n",
            "train loss: 205.256943  [   64/ 5000]\n",
            "train loss: 168.024292  [   64/ 5000]\n",
            "train loss: 150.244339  [   64/ 5000]\n",
            "train loss: 160.176086  [   64/ 5000]\n",
            "train loss: 184.856110  [   64/ 5000]\n",
            "train loss: 173.721237  [   64/ 5000]\n",
            "train loss: 124.296471  [   64/ 5000]\n",
            "train loss: 191.843704  [   64/ 5000]\n",
            "train loss: 134.630615  [   64/ 5000]\n",
            "train loss: 136.323242  [   64/ 5000]\n",
            "train loss: 119.988464  [   64/ 5000]\n",
            "train loss: 202.928391  [   64/ 5000]\n",
            "Test Error: \n",
            " Accuracy: 25.9%, Avg loss: 124.407315 \n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-83ea344a8593>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    225\u001b[0m   \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 227\u001b[0;31m   \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_accuracy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    229\u001b[0m   \u001b[0maxs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mylabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'test accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'test_accuracy' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "#TODO: make the inputs 3 channels instead of 3072 individual values\n",
        "#TODO: complete grid search over hyperparameters\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "TRAIN_SIZE = 5000\n",
        "TEST_SIZE = 1000\n",
        "\n",
        "\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True, # set to true for first run - TODO: find elegant solution\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,# set to true for first run - TODO: find elegant solution\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "def svm_by_type(kernel , X , y , X_test , y_test):\n",
        "  clf_SVC = SVC(kernel=kernel , decision_function_shape='ovr')\n",
        "  clf_SVC.fit(X.reshape([TRAIN_SIZE, 3 * 32 * 32]), y)\n",
        "  print(\"train acc: \", clf_SVC.score(X.reshape([TRAIN_SIZE, 3 * 32 * 32]), y))\n",
        "  print(\"test acc: \", clf_SVC.score(X_test.reshape([TEST_SIZE, 3 * 32 * 32]), y_test))\n",
        "  print(\"number of classes: \", clf_SVC.classes_)\n",
        "  \n",
        "  \n",
        "# TODO: should choose parameters here\n",
        "config = {\n",
        "    \"step_size\": [1e-3 , 1e-2 , 2e-2 , 3e-2 , 0.1],\n",
        "    \"momentum\": [ 0.45 , 0.5 , 0.55 , 0.7 , 0.8 , 0.9],\n",
        "    \"deviation\": [ 1 , 1.5 , 2 , 2.25 , 2.5 ,  3]\n",
        "}\n",
        "\n",
        "\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(3*32*32, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "      \n",
        "\n",
        "def init_weights_wrapper(deviation):\n",
        "  def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "      torch.nn.init.normal_(m.weight , std=deviation)\n",
        "      torch.nn.init.normal_(m.bias , std=deviation)\n",
        "  return init_weights\n",
        "\n",
        "\n",
        "class GridSearch():\n",
        "  def __init__(self):\n",
        "    self.best_params = {}\n",
        "    self.best_acc = 0\n",
        "\n",
        "  def update_params(self, step_size, momentum, deviation):\n",
        "    self.best_params['step_size'] = step_size\n",
        "    self.best_params['momentum'] = momentum\n",
        "    self.best_params['deviation'] = deviation\n",
        "\n",
        "  def run(self, train_dataloader, config):\n",
        "    for step_size in config[\"step_size\"]:\n",
        "      for momentum in config[\"momentum\"]:\n",
        "        for deviation in config[\"deviation\"]:\n",
        "          model = NeuralNetwork().to(device)\n",
        "          model.apply(init_weights_wrapper(deviation))\n",
        "\n",
        "          loss_fn = nn.CrossEntropyLoss()\n",
        "          optimizer = torch.optim.SGD(model.parameters(), lr=step_size , momentum=momentum)\n",
        "          all_accuracy, _ = train(train_dataloader, model, loss_fn, optimizer )\n",
        "          accuracy = all_accuracy[-1] # Take last accuracy\n",
        "          if accuracy > self.best_acc:\n",
        "            self.update_params(step_size, momentum, deviation)\n",
        "            self.best_acc = accuracy\n",
        "\n",
        "  \n",
        "# Returns the accuracy\n",
        "def train(dataloader, model, loss_fn, optimizer , num_of_batches = -1, num_of_epochs=50):\n",
        "    size = TRAIN_SIZE #TODO: change size to match num_of_batches\n",
        "    model.train()\n",
        "    epoch_loss, epoch_accuracy = 0, 0\n",
        "    all_loss, all_accuracy = [], [] # loss and accuracy per epoch\n",
        "    for i in range(num_of_epochs):\n",
        "      for batch, (X, y) in enumerate(dataloader):\n",
        "          X, y = X.to(device), y.to(device)\n",
        "\n",
        "          # Compute prediction error\n",
        "          pred = model(X)\n",
        "          loss = loss_fn(pred, y)\n",
        "\n",
        "          # Backpropagation\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          loss = loss.item()\n",
        "          if batch % 100 == 0:\n",
        "              current = (batch + 1) * len(X)\n",
        "              print(f\"train loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
        "          epoch_loss += loss\n",
        "          epoch_accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "          if num_of_batches == 0 :\n",
        "            break\n",
        "          else:\n",
        "            num_of_batches -=1\n",
        "      all_accuracy.append(epoch_accuracy / size)\n",
        "      if (num_of_batches > -1):\n",
        "        all_loss.append(epoch_loss / num_of_batches)\n",
        "      else:\n",
        "        all_loss.append(epoch_loss / len(dataloader))\n",
        "      \n",
        "      epoch_accuracy = 0\n",
        "      epoch_loss = 0\n",
        "    #all_loss /= num_batches\n",
        "    return all_accuracy, all_loss\n",
        "\n",
        "\n",
        "def test(dataloader, model, loss_fn , num_of_batches = -1):\n",
        "    size = TEST_SIZE #len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "            if num_of_batches == 0 :\n",
        "              break\n",
        "            else:\n",
        "              num_of_batches -=1\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  train_sampler = torch.utils.data.RandomSampler(training_data, replacement=True, num_samples=TRAIN_SIZE, generator=None)\n",
        "  test_sampler = torch.utils.data.RandomSampler(test_data, replacement=True, num_samples=TEST_SIZE, generator=None)\n",
        "\n",
        "\n",
        "\n",
        "  # Loaders for SVM (load everything)\n",
        "  train_dataloader = DataLoader(training_data, batch_size=TRAIN_SIZE , sampler = train_sampler)\n",
        "  test_dataloader = DataLoader(test_data, batch_size=TEST_SIZE, sampler = test_sampler)\n",
        "\n",
        "  X , y = next(iter(train_dataloader))\n",
        "  X_test , y_test = next(iter(test_dataloader))\n",
        "\n",
        "  # linear SVM:\n",
        "  #svm_by_type('linear' , X , y , X_test , y_test)\n",
        "\n",
        "  \n",
        "\n",
        "  # rbf kernel SVM\n",
        "  #svm_by_type('rbf' , X , y , X_test , y_test)\n",
        "\n",
        "  # Loaders for NN\n",
        "  train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE , sampler = train_sampler)\n",
        "  test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE, sampler = test_sampler)\n",
        "\n",
        "  # Choose best parameters\n",
        "  \"\"\"\n",
        "  gs = GridSearch()\n",
        "  gs.run(train_dataloader, config)\n",
        "  best_step_size = gs.best_params[\"step_size\"]\n",
        "  best_momentum = gs.best_params[\"momentum\"]\n",
        "  best_deviation =  gs.best_params[\"deviation\"]\n",
        "  \"\"\"\n",
        "\n",
        "  best_step_size = 0.001\n",
        "  best_momentum = 0.5\n",
        "  best_deviation = 2.5\n",
        "\n",
        "  print(best_step_size)\n",
        "  print(best_momentum)\n",
        "  print(best_deviation)\n",
        "\n",
        "\n",
        "  # Train the network with the best parameters and then test it\n",
        "  model = NeuralNetwork().to(device)\n",
        "  model.apply(init_weights_wrapper(best_deviation))\n",
        "  for name, param in model.named_parameters():\n",
        "          print(name , param)\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=best_step_size , momentum=best_momentum)\n",
        "  all_accuracy, all_loss = train(train_dataloader, model, loss_fn, optimizer)\n",
        "  epochs = [(x + 1) for x in range(len(all_accuracy))]\n",
        "  fig, axs = plt.subplots(2)\n",
        "  fig.suptitle(\"Accuracy as func of epochs\")\n",
        "  # Plot\n",
        "  axs[0].plot(epochs, all_accuracy)\n",
        "\n",
        "  axs[0].set(xlabel='epochs', ylabel='train accuracy')\n",
        "  axs[0].grid()\n",
        "\n",
        "  test(test_dataloader, model, loss_fn)\n",
        "\n",
        "  axs[1].plot(epochs, test_accuracy)\n",
        "\n",
        "  axs[1].set(xlabel='epochs', ylabel='test accuracy')\n",
        "  axs[1].grid()\n",
        "\n",
        "  save = False\n",
        "  if save:\n",
        "      plt.savefig(\"title for plt\")\n",
        "      \n",
        "  # Draw losses as well\n",
        "  plt.show()\n",
        "  print(\"end\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This isn't relevant (didn't work)\n",
        "# TODO: delete\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(f\"Using {device} device\")\n",
        "\n",
        "BATCH_SIZE = 64\n",
        "TRAIN_SIZE = 5000\n",
        "TEST_SIZE = 1000\n",
        "\n",
        "\n",
        "config = {\n",
        "    \"step_size\": [1e-3],\n",
        "    \"momentum\": [0.9],\n",
        "    \"deviation\": [1]\n",
        "}\n",
        "\n",
        "\n",
        "training_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=True,\n",
        "    download=True, # set to true for first run - TODO: find elegant solution\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "\n",
        "# Download test data from open datasets.\n",
        "test_data = datasets.CIFAR10(\n",
        "    root=\"data\",\n",
        "    train=False,\n",
        "    download=True,# set to true for first run - TODO: find elegant solution\n",
        "    transform=ToTensor(),\n",
        ")\n",
        "class NeuralNetwork(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.linear_relu_stack = nn.Sequential(\n",
        "            nn.Linear(3*32*32, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 10)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        logits = self.linear_relu_stack(x)\n",
        "        return logits\n",
        "\n",
        "      \n",
        "\n",
        "def init_weights_wrapper(deviation):\n",
        "  def init_weights(m):\n",
        "    if isinstance(m, nn.Linear):\n",
        "      torch.nn.init.normal_(m.weight , std=deviation)\n",
        "      torch.nn.init.normal_(m.bias , std=deviation)\n",
        "  return init_weights\n",
        "\n",
        "def search_params(step_size, momentum, deviation):\n",
        "  model = NeuralNetwork().to(device)\n",
        "  model.apply(init_weights_wrapper(deviation))\n",
        "\n",
        "  loss_fn = nn.CrossEntropyLoss()\n",
        "  optimizer = torch.optim.SGD(model.parameters(), lr=step_size , momentum=momentum)\n",
        "  train(train_dataloader, model, loss_fn, optimizer)\n",
        "\n",
        "\n",
        "\n",
        "class ParameterSearch():\n",
        "  def  __init__(self):\n",
        "    pass\n",
        "  def fit(self):\n",
        "    pass\n",
        "  def score(self):\n",
        "    return 0\n",
        "\n",
        "\n",
        "gs = GridSearchCV(ParameterSearch(),  param_grid=config #,\n",
        "        #refit=False, scoring='accuracy', verbose=1, cv=2\n",
        "    )\n",
        "train_sampler = torch.utils.data.RandomSampler(training_data, replacement=True, num_samples=TRAIN_SIZE, generator=None)\n",
        "test_sampler = torch.utils.data.RandomSampler(test_data, replacement=True, num_samples=TEST_SIZE, generator=None)\n",
        "train_dataloader = DataLoader(training_data, batch_size=BATCH_SIZE , sampler = train_sampler)\n",
        "for i, data in enumerate(train_dataloader):\n",
        "    image, labels = data\n",
        "    image = image.to(device)\n",
        "    labels = labels.to(device)\n",
        "    outputs = gs.fit(image, labels)\n",
        "\n"
      ],
      "metadata": {
        "id": "s9gKKLOCZGTS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 433
        },
        "outputId": "00fa2d29-f5ce-46b3-b646-9683bbd50446"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-80a8178fd1bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    786\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv_orig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 788\u001b[0;31m         \u001b[0mbase_estimator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    789\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    790\u001b[0m         \u001b[0mparallel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dispatch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     77\u001b[0m                 )\n\u001b[1;32m     78\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m                 raise TypeError(\n\u001b[0m\u001b[1;32m     80\u001b[0m                     \u001b[0;34m\"Cannot clone object '%s' (type %s): \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0;34m\"it does not seem to be a scikit-learn \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Cannot clone object '<__main__.ParameterSearch object at 0x7f2d68e1e250>' (type <class '__main__.ParameterSearch'>): it does not seem to be a scikit-learn estimator as it does not implement a 'get_params' method."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aL5WczYWTEgy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}